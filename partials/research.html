<div style="margin: 50px">

    <div class="panel panel-default">
        <div class="panel-heading bg-primary">
            <h3 class="panel-title">Articulatory Gesture Rich Representation Learning of Phonological Units in Low Resource Settings</h3>
        </div>
        <div class="panel-body">
            <h6>Role: Research Scholar</h6>

            <h6>Accepted at Statistical Language and Speech Processing 2016, Pilzen, Czech Republic. (SLSP 2016)</h6>
            <a href="http://link.springer.com/chapter/10.1007/978-3-319-45925-7_7">[PDF]</a><a href="https://scholar.googleusercontent.com/scholar.bib?q=info:gryOaizqyfAJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAWGroXYpzYOjMSzzYVcAJO7NwVPipJ1Mg&scisf=4&ct=citation&cd=-1&hl=en">[BIB]</a><br>

            <img src="files/slsp_approach.png" width="700px">

            <p>Recent literature presents evidence that both linguistic (phonemic) and non linguistic (speaker identity, emotional content) information resides at a lower dimensional manifold embedded richly inside the higher-dimensional spectral features like MFCC and PLP. Linguistic or phonetic units of speech can be broken down to a legal inventory of articulatory gestures shared across several phonemes based on their manner of articulation. We intend to discover a subspace rich in gestural information of speech and captures the invariance of similar gestures. In this paper, we investigate unsupervised techniques best suited for learning such a subspace. Main contribution of the paper is an approach to learn gesture-rich representation of speech automatically from data in completely unsupervised manner. This study compares the representations obtained through convolutional autoencoder (ConvAE) and standard unsupervised dimensionality reduction techniques such as manifold learning and Principal Component Analysis (PCA) through the task of phoneme classification. Manifold learning techniques such as Locally Linear Embedding (LLE), Isomap and Laplacian Eigenmaps are evaluated in this study. The representations which best separate different gestures are suitable for discovering subword units in case of low or zero resource speech conditions. Further, we evaluate the representation using Zero Resource Speech Challenge’s ABX discriminability measure. Results indicate that representation obtained through ConvAE and Isomap out-perform baseline MFCC features in the task of phoneme classification as well as ABX measure and induce separation between sounds composed of different set of gestures. We further cluster the representations using Dirichlet Process Gaussian Mixture Model (DPGMM) to automatically learn the cluster distribution of data and show that these clusters correspond to groups of similar manner of articulation. DPGMM distribution is used as apriori to obtain correspondence terms for robust ConvAE training.</p>
        </div>
    </div>

    <div class="panel panel-default">
        <div class="panel-heading bg-primary">
            <h3 class="panel-title">Vaidya: A multilingual spoken dialog system for Indian healthcare scenarios</h3>
        </div>
        <div class="panel-body">
            <h6>Role: Research Scholar</h6>

            <h6>Accepted at International Conference in NLP (ICON 2016)</h6>
            <a href="files/vaidya-spoken-dialog.pdf">[PDF]</a><a href="files/W16-5121.bib">[BIB]</a><br>

            <img src="files/vaidya_arch_final.png" width="700px">

            <p>In this paper, we introduce Vaidya, a spoken dialog system which is developed as
                part of the ITRA project. The system is
                capable of providing an approximate diagnosis by accepting symptoms as free-
                form speech in real-time on both laptop
                and hand-held devices. The system focuses on challenges in speech recognition
                specific to Indian languages and capturing
                the intent of the user. Another challenge
                is to create models which are memory and
                CPU efficient for hand-held devices. We
                describe our progress, experiences and approaches in building the system that can
                handle English as the input speech. The
                system is evaluated using subjective statistical measure (Fleiss’ kappa) to assess the
                usability of the system.</p>
        </div>
    </div>

    <div class="panel panel-default">
        <div class="panel-heading bg-primary">
            <h3 class="panel-title">Significance of neural phonotactic models for large-scale spoken language identification</h3>
        </div>
        <div class="panel-body">
            <h6>Role: Research Scholar</h6>

            <h6>Submitted to IJCNN 2017 - Under Review</h6>

            <img src="files/lid_arc_conpt.png" width="700px">

            <p>Multilingual spoken dialogue systems have gained prominence in the
                recent past necessitating the requirement for a front-end Language
                Identification (LID) system. Most of the existing LID systems rely
                on modeling the language discriminative information from low-level
                acoustic features. Due to the variabilities of speech (speaker and
                emotional variabilities, etc.), large-scale LID systems developed
                using low-level acoustic features suffer from a degradation in the
                performance. In this approach, we have attempted to model the higher
                level language discriminative phonotactic information for developing
                an LID system. In this paper, the input speech signal is tokenized to
                phone sequences by using a language independent phone recognizer. The
                language discriminative phonotactic information in the obtained phone
                sequences are modeled using statistical and recurrent neural network
                based language modeling approaches. As this approach, relies on higher
                level phonotactical information it is more robust to variabilities of
                speech. Proposed approach is computationally light weight, highly scalable
                and it can be used in complement with the existing LID systems.</p>
        </div>
    </div>

    <div class="panel panel-default">
        <div class="panel-heading bg-primary">
            <h3 class="panel-title">Eating Conditions Classification using Convolutional Neural Networks</h3>
        </div>
        <div class="panel-body">
            <h6>Role: Research Scholar</h6>

            <h6>Submitted to INTERSPEECH 2015 - Under Review</h6>
            
            <img src="imgs/cnn_diag.png">

            <p>Chewing is a vital process for health and dietary monitoring.
                Each food has a unique composition which affects the chew
                ing process. However, we can harness this property in order to
                detect the food being chewed while speech is produced. The
                Interspeech 2015 Eating Conditions SubChallenge involves
                classification of speech samples into 7 classes of food (including
                N oF ood). In this paper, we report our observations wherein
                we experiment with various feature sets along with varying
                architectures of CNN in order to maximize the accuracy of
                classification. Finally, we present an analysis of the architecture
                which outperforms the baseline dramatically.</p>
        </div>
    </div>

    <div class="panel panel-default">
        <div class="panel-heading bg-primary">
            <h3 class="panel-title"><i>bRead</i>: A Framework for Expressive Audiobooks Generation from Text</h3>
        </div>
        <div class="panel-body">
            <h6>Role: Research Scholar</h6>

            <h6>Submitted to INTERSPEECH 2015 - Under Review</h6>

            <img src="imgs/bread_framework.png">

            <p>Audiobooks are a great source of expressive speech data. Often, in case of TTS,
                we rely on read speech audiobooks to model
                pitch and duration since it improves the naturalness in speech.
                Audiobooks also helps preserving the ancient literary works and
                can be further used to build ASR. Audio books are usually read
                by single speaker. For more impactful audiobooks, we propose
                a framework, bRead (pronounced as ”bread”), which creates
                multi-speaker audiobooks from text. In order to generate
                expressive speech, which respects the course of the story, we need
                to extract a defined set of information from the given text. This
                information should contain details of each character involved in
                the story, eg. gender, age, personality, etc. Further we should
                be aware of the parts of text spoken by each character and the
                tone which prevailed over those parts. We will adopt several
                NLP techniques in order to extract this information from the
                text. We will further explain the TTS strategies we followed in
                order to achieve best possible outcome. Finally, we explain the
                techniques we used for the evaluation of this framework.</p>

        </div>
    </div>

    <div class="panel panel-default">
        <div class="panel-heading bg-primary">
            <h3 class="panel-title">A Framework for Humor Recognition from Social Media using Word Embeddings</h3>
        </div>
        <div class="panel-body">
            <h6>Role: Research Scholar</h6>

            <h6>Submitted to SIGIR 2015 - Under Review</h6>

            <p>We propose a word embedding approach for the recognition
            of humour in social media data. Conventional methods aim
            at modeling the text structure and hand crafting features
            surgically designed to emulate linguistic tendencies in specific
            types of jokes. We try to take a more hands off approach
            and use a data driven approach based on the word embeddings
            to design a complete framework. The experimental
            results show substantial improvements and outperform the
            baseline.</p>


            <p>In the process of gaining experience, our brain collects
            memories and trains itself to predict the outcome of events
            beforehand. For example, if a person is moving towards the
            door, we expect him to open/close the door. We do not
            realize but the cognitive model of brain prepares itself with a
            few choices, and that forms the obvious or expected turn of
            events. With this prior information, we can define ”humor”
            as the turn of events which are unexpected by this cognitive
            model. As soon as there is an event which do not follow the
            predictions of brain, we experience a peculiar sort of muscle
            movement which can be characterized by laughter, giggle or
            smile. We are trying to capture the ambiguity that we just
            mentioned, by suitably modeling the words as vectors in a
            higher dimensional space and thus come up with a classifier
            which can distinguish between humorous and non-humorous
            sentences based on the distribution of the word vectors.</p>
        </div>
    </div>

</div>